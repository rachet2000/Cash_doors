tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep <- R_2(nnet.C.rep, y_C)
## third run
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
## fourth run
## iterative search for convergence
if ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr )  {
while ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr ) {
nnet.C <- nnet.C.rep
R2_nnet <- R2_nnet.rep
nnet.C.rep <- nnet.C.rep2
R2_nnet.rep <- R2_nnet.rep2
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try (hidden layers=2,3,4; decay parameter=0.0001,0.001,0.01,0.1,1):
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
}
} else nnet.C.rep2 = nnet.C.rep2
nnet.C <- nnet.C.rep2
return(nnet.C)
}
main <- function(){
load()
n.iter <- 1		## number of datasets
n.perm <- 3		## number of permutations to calculated R^2 reduction
n.samp <- 100		## sample size
sd.val = 0.1			## error standard deviation
tlr = 0.05			## tolerance of convergence algorithm
dat <- getdata(n.samp, sd.val)
results.mat <- foreach (i = 1:n.iter, .combine=rbind, .packages=c("nnet", "caret")) %dorng% {
print(i)
f <- formula(y_C ~ x1.sc + x2.sc + x3.sc)
perm.results.mat <- matrix(NA, nrow=n.perm, ncol=6)		## set up matrix for the recording of simulation results
for (j in 1:n.perm) {
x1.sc.perm <- sample(dat$x1.sc)				# permute each predictor variable to break the association (if any) between the predictor variable and the response
x2.sc.perm <- sample(dat$x2.sc)
x3.sc.perm <- sample(dat$x3.sc)
perm1 <- dat
perm2 <- dat
perm3 <- dat
perm12 <- dat
perm23 <- dat
perm31 <- dat
perm1$x1.sc <- x1.sc.perm
perm2$x2.sc <- x2.sc.perm
perm3$x3.sc <- x3.sc.perm
perm12$x1.sc <- x1.sc.perm
perm12$x2.sc <- x2.sc.perm
perm23$x2.sc <- x2.sc.perm
perm23$x3.sc <- x3.sc.perm
perm31$x3.sc <- x3.sc.perm
perm31$x1.sc <- x1.sc.perm
nnet.C <- convergence(f, dat, tlr)
nnet.C.x1_perm <- convergence(f, perm1, tlr)
nnet.C.x2_perm <- convergence(f, perm2, tlr)
nnet.C.x3_perm <- convergence(f, perm3, tlr)
nnet.C.x12_perm <- convergence(f, perm12, tlr)
nnet.C.x23_perm <- convergence(f, perm23, tlr)
nnet.C.x31_perm <- convergence(f, perm31, tlr)
#nnet.C.x12_rem <- convergence(y_C ~ x3.sc, perm12, tlr)
#nnet.C.x23_rem <- convergence(y_C ~ x1.sc, perm23, tlr)
#nnet.C.x31_rem <- convergence(y_C ~ x2.sc, perm31, tlr)
nnet.x1_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x1_perm, dat$y_C)
nnet.x2_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x2_perm, dat$y_C)
nnet.x3_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x3_perm, dat$y_C)
nnet.x12_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_perm, dat$y_C)
nnet.x23_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_perm, dat$y_C)
nnet.x31_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_perm, dat$y_C)
#nnet.x12r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_rem, dat$y_C)
#nnet.x23r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_rem, dat$y_C)
#nnet.x31r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_rem, dat$y_C)
perm.results.mat[j,] <- c(nnet.x1_Rsq_redu, nnet.x2_Rsq_redu, nnet.x3_Rsq_redu,
nnet.x12_Rsq_redu, nnet.x23_Rsq_redu, nnet.x31_Rsq_redu)
}
return(c(colMeans(perm.results.mat),
apply(perm.results.mat, 2, FUN=median),
R_2(nnet.C, dat$y_C),
nnet.C$finalModel$convergence, as.numeric(nnet.C$bestTune)))
}
results<- data.frame(results.mat)
# names(results) <- c("X1_nnet_RsqRed", "X2_nnet_RsqRed","X3_nnet_RsqRed",
#                               "X12_nnet_RsqRed", "X23_nnet_RsqRed","X31_nnet_RsqRed",
#                              "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                             "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                            "NN_Rsq",
#                           "Convergence", "Size", "Decay")
write.csv(results, 'results_standard.csv')
return(results)
}
load <- function(){library(nnet)
library(NeuralNetTools)
library(caret)
library(doParallel)
library(doRNG)
library(foreach)}
R_2 <- function(model, resp){
1 - sum((predict(model)-resp)^2)/sum((resp-mean(resp))^2)
}
getdata <- function(n.samp, sd.val){
mu <- rep(0,3)
Sigma <- matrix(.3, nrow=3, ncol=3) + diag(3)*.7
rawvars <- mvrnorm(n=n.samp, mu=mu, Sigma=Sigma)
x1.sc <- scale(rawvars[,1])			# center and scale predictor variables
x2.sc <- scale(rawvars[,2])
x3.sc <- scale(rawvars[,3])
y_C <- -1*(x1.sc^2) + 0.5*x2.sc + 0.1*x3.sc + rnorm(n.samp,0,sd.val)		# generate response variable y_C
dat <- data.frame(y_C, x1.sc, x2.sc, x3.sc)
return(dat)
}
convergence <- function(formula, dat, tlr){ ############################################ Start of convergence algorithm ###################################################
y_C <- dat$y_C
## fit 4 separate ANNs and calculate R^2 of ANN model
## accept the 4th model as final model if R^2 range of 4 consecutive models does not exceed 0.01
## if the R^2 range of 4 consecutive models exceed 0.01, run new models until 4 consecutive models does not exceed 0.01
## first run
nnet.C <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet <- R_2(nnet.C, y_C)
## second run
nnet.C.rep <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep <- R_2(nnet.C.rep, y_C)
## third run
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
## fourth run
## iterative search for convergence
if ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr )  {
while ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr ) {
nnet.C <- nnet.C.rep
R2_nnet <- R2_nnet.rep
nnet.C.rep <- nnet.C.rep2
R2_nnet.rep <- R2_nnet.rep2
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try (hidden layers=2,3,4; decay parameter=0.0001,0.001,0.01,0.1,1):
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
}
} else nnet.C.rep2 = nnet.C.rep2
nnet.C <- nnet.C.rep2
return(nnet.C)
}
results_corr <- main()
warnings()
main <- function(){
load()
n.iter <- 15		## number of datasets
n.perm <- 3		## number of permutations to calculated R^2 reduction
n.samp <- 100		## sample size
sd.val = 0.1			## error standard deviation
tlr = 0.05			## tolerance of convergence algorithm
dat <- getdata(n.samp, sd.val)
results.mat <- foreach (i = 1:n.iter, .combine=rbind, .packages=c("nnet", "caret")) %dorng% {
print(i)
f <- formula(y_C ~ x1.sc + x2.sc + x3.sc)
perm.results.mat <- matrix(NA, nrow=n.perm, ncol=6)		## set up matrix for the recording of simulation results
for (j in 1:n.perm) {
x1.sc.perm <- sample(dat$x1.sc)				# permute each predictor variable to break the association (if any) between the predictor variable and the response
x2.sc.perm <- sample(dat$x2.sc)
x3.sc.perm <- sample(dat$x3.sc)
perm1 <- dat
perm2 <- dat
perm3 <- dat
perm12 <- dat
perm23 <- dat
perm31 <- dat
perm1$x1.sc <- x1.sc.perm
perm2$x2.sc <- x2.sc.perm
perm3$x3.sc <- x3.sc.perm
perm12$x1.sc <- x1.sc.perm
perm12$x2.sc <- x2.sc.perm
perm23$x2.sc <- x2.sc.perm
perm23$x3.sc <- x3.sc.perm
perm31$x3.sc <- x3.sc.perm
perm31$x1.sc <- x1.sc.perm
nnet.C <- convergence(f, dat, tlr)
nnet.C.x1_perm <- convergence(f, perm1, tlr)
nnet.C.x2_perm <- convergence(f, perm2, tlr)
nnet.C.x3_perm <- convergence(f, perm3, tlr)
nnet.C.x12_perm <- convergence(f, perm12, tlr)
nnet.C.x23_perm <- convergence(f, perm23, tlr)
nnet.C.x31_perm <- convergence(f, perm31, tlr)
#nnet.C.x12_rem <- convergence(y_C ~ x3.sc, perm12, tlr)
#nnet.C.x23_rem <- convergence(y_C ~ x1.sc, perm23, tlr)
#nnet.C.x31_rem <- convergence(y_C ~ x2.sc, perm31, tlr)
nnet.x1_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x1_perm, dat$y_C)
nnet.x2_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x2_perm, dat$y_C)
nnet.x3_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x3_perm, dat$y_C)
nnet.x12_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_perm, dat$y_C)
nnet.x23_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_perm, dat$y_C)
nnet.x31_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_perm, dat$y_C)
#nnet.x12r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_rem, dat$y_C)
#nnet.x23r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_rem, dat$y_C)
#nnet.x31r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_rem, dat$y_C)
perm.results.mat[j,] <- c(nnet.x1_Rsq_redu, nnet.x2_Rsq_redu, nnet.x3_Rsq_redu,
nnet.x12_Rsq_redu, nnet.x23_Rsq_redu, nnet.x31_Rsq_redu)
}
return(c(colMeans(perm.results.mat),
apply(perm.results.mat, 2, FUN=median),
R_2(nnet.C, dat$y_C),
nnet.C$finalModel$convergence, as.numeric(nnet.C$bestTune)))
}
results<- data.frame(results.mat)
# names(results) <- c("X1_nnet_RsqRed", "X2_nnet_RsqRed","X3_nnet_RsqRed",
#                               "X12_nnet_RsqRed", "X23_nnet_RsqRed","X31_nnet_RsqRed",
#                              "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                             "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                            "NN_Rsq",
#                           "Convergence", "Size", "Decay")
write.csv(results, 'results_standard.csv')
return(results)
}
load <- function(){library(nnet)
library(NeuralNetTools)
library(caret)
library(doParallel)
library(doRNG)
library(foreach)}
R_2 <- function(model, resp){
1 - sum((predict(model)-resp)^2)/sum((resp-mean(resp))^2)
}
getdata <- function(n.samp, sd.val){
mu <- rep(0,3)
Sigma <- matrix(.3, nrow=3, ncol=3) + diag(3)*.7
rawvars <- mvrnorm(n=n.samp, mu=mu, Sigma=Sigma)
x1.sc <- scale(rawvars[,1])			# center and scale predictor variables
x2.sc <- scale(rawvars[,2])
x3.sc <- scale(rawvars[,3])
y_C <- -1*(x1.sc^2) + 0.5*x2.sc + 0.1*x3.sc + rnorm(n.samp,0,sd.val)		# generate response variable y_C
dat <- data.frame(y_C, x1.sc, x2.sc, x3.sc)
return(dat)
}
convergence <- function(formula, dat, tlr){ ############################################ Start of convergence algorithm ###################################################
y_C <- dat$y_C
## fit 4 separate ANNs and calculate R^2 of ANN model
## accept the 4th model as final model if R^2 range of 4 consecutive models does not exceed 0.01
## if the R^2 range of 4 consecutive models exceed 0.01, run new models until 4 consecutive models does not exceed 0.01
## first run
nnet.C <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet <- R_2(nnet.C, y_C)
## second run
nnet.C.rep <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep <- R_2(nnet.C.rep, y_C)
## third run
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
## fourth run
## iterative search for convergence
if ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr )  {
while ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr ) {
nnet.C <- nnet.C.rep
R2_nnet <- R2_nnet.rep
nnet.C.rep <- nnet.C.rep2
R2_nnet.rep <- R2_nnet.rep2
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try (hidden layers=2,3,4; decay parameter=0.0001,0.001,0.01,0.1,1):
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
}
} else nnet.C.rep2 = nnet.C.rep2
nnet.C <- nnet.C.rep2
return(nnet.C)
}
results_corr <- main()
results_standard
results_removed[1]
results_rem[1]
results_remove[1]
results_standard[1][1]
results_standard[1][1]
results_corr
write.csv(results.remove, "results_remove.csv")
write.csv(results_remove, "results_remove.csv")
write.csv(results_corr, "results_corr.csv")
?Boston
anova(lm(medv~ lstat + black + lstat*black, data = Boston))
anova(lm(medv~ lstat + black + lstat*black, data = Boston))
anova(lm(medv~ rm + black + rm*black, data = Boston))
dat <- getdata(n.samp, sd.val)
results.mat <- foreach (i = 1:n.iter, .combine=rbind, .packages=c("nnet", "caret")) %dorng% {
print(i)
f <- formula(y_C ~ x1.sc + x2.sc + x3.sc)
perm.results.mat <- matrix(NA, nrow=n.perm, ncol=6)		## set up matrix for the recording of simulation results
for (j in 1:n.perm) {
x1.sc.perm <- sample(dat$x1.sc)				# permute each predictor variable to break the association (if any) between the predictor variable and the response
x2.sc.perm <- sample(dat$x2.sc)
x3.sc.perm <- sample(dat$x3.sc)
perm1 <- dat
perm2 <- dat
perm3 <- dat
perm12 <- dat
perm23 <- dat
perm31 <- dat
perm1$x1.sc <- x1.sc.perm
perm2$x2.sc <- x2.sc.perm
perm3$x3.sc <- x3.sc.perm
perm12$x1.sc <- x1.sc.perm
perm12$x2.sc <- x2.sc.perm
perm23$x2.sc <- x2.sc.perm
perm23$x3.sc <- x3.sc.perm
perm31$x3.sc <- x3.sc.perm
perm31$x1.sc <- x1.sc.perm
nnet.C <- convergence(f, dat, tlr)
nnet.C.x1_perm <- convergence(f, perm1, tlr)
nnet.C.x2_perm <- convergence(f, perm2, tlr)
nnet.C.x3_perm <- convergence(f, perm3, tlr)
nnet.C.x12_perm <- convergence(f, perm12, tlr)
nnet.C.x23_perm <- convergence(f, perm23, tlr)
nnet.C.x31_perm <- convergence(f, perm31, tlr)
#nnet.C.x12_rem <- convergence(y_C ~ x3.sc, perm12, tlr)
#nnet.C.x23_rem <- convergence(y_C ~ x1.sc, perm23, tlr)
#nnet.C.x31_rem <- convergence(y_C ~ x2.sc, perm31, tlr)
nnet.x1_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x1_perm, dat$y_C)
nnet.x2_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x2_perm, dat$y_C)
nnet.x3_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x3_perm, dat$y_C)
nnet.x12_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_perm, dat$y_C)
nnet.x23_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_perm, dat$y_C)
nnet.x31_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_perm, dat$y_C)
#nnet.x12r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x12_rem, dat$y_C)
#nnet.x23r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x23_rem, dat$y_C)
#nnet.x31r_Rsq_redu<-R_2(nnet.C, dat$y_C) - R_2(nnet.C.x31_rem, dat$y_C)
perm.results.mat[j,] <- c(nnet.x1_Rsq_redu, nnet.x2_Rsq_redu, nnet.x3_Rsq_redu,
nnet.x12_Rsq_redu, nnet.x23_Rsq_redu, nnet.x31_Rsq_redu)
}
return(c(colMeans(perm.results.mat),
apply(perm.results.mat, 2, FUN=median),
R_2(nnet.C, dat$y_C),
nnet.C$finalModel$convergence, as.numeric(nnet.C$bestTune)))
}
results<- data.frame(results.mat)
# names(results) <- c("X1_nnet_RsqRed", "X2_nnet_RsqRed","X3_nnet_RsqRed",
#                               "X12_nnet_RsqRed", "X23_nnet_RsqRed","X31_nnet_RsqRed",
#                              "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                             "X1_nnet_RsqRed_med", "X2_nnet_RsqRed_med","X3_nnet_RsqRed_med",
#                            "NN_Rsq",
#                           "Convergence", "Size", "Decay")
write.csv(results, 'results_standard.csv')
return(results)
}
load <- function(){library(nnet)
library(NeuralNetTools)
library(caret)
library(doParallel)
library(doRNG)
library(foreach)}
R_2 <- function(model, resp){
1 - sum((predict(model)-resp)^2)/sum((resp-mean(resp))^2)
}
getdata <- function(n.samp, sd.val){
mu <- rep(0,3)
Sigma <- matrix(.3, nrow=3, ncol=3) + diag(3)*.7
rawvars <- mvrnorm(n=n.samp, mu=mu, Sigma=Sigma)
x1.sc <- scale(rawvars[,1])			# center and scale predictor variables
x2.sc <- scale(rawvars[,2])
x3.sc <- scale(rawvars[,3])
y_C <- -1*(x1.sc^2) + 0.5*x2.sc + 0.1*x3.sc + rnorm(n.samp,0,sd.val)		# generate response variable y_C
dat <- data.frame(y_C, x1.sc, x2.sc, x3.sc)
return(dat)
}
convergence <- function(formula, dat, tlr){ ############################################ Start of convergence algorithm ###################################################
y_C <- dat$y_C
## fit 4 separate ANNs and calculate R^2 of ANN model
## accept the 4th model as final model if R^2 range of 4 consecutive models does not exceed 0.01
## if the R^2 range of 4 consecutive models exceed 0.01, run new models until 4 consecutive models does not exceed 0.01
## first run
nnet.C <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet <- R_2(nnet.C, y_C)
## second run
nnet.C.rep <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep <- R_2(nnet.C.rep, y_C)
## third run
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
## fourth run
## iterative search for convergence
if ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr )  {
while ( (max(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2)) - min(c(R2_nnet, R2_nnet.rep, R2_nnet.rep2))) > tlr ) {
nnet.C <- nnet.C.rep
R2_nnet <- R2_nnet.rep
nnet.C.rep <- nnet.C.rep2
R2_nnet.rep <- R2_nnet.rep2
nnet.C.rep2 <- train(formula, data=dat, method='nnet', maxit=10000, linout=TRUE, trace = F,
trControl = trainControl(method="cv"),
#Grid of tuning parameters to try (hidden layers=2,3,4; decay parameter=0.0001,0.001,0.01,0.1,1):
tuneGrid=expand.grid(.size=c(2,3,4),.decay=c(0.001,0.01,0.1)))
R2_nnet.rep2 <- R_2(nnet.C.rep2, y_C)
}
} else nnet.C.rep2 = nnet.C.rep2
nnet.C <- nnet.C.rep2
return(nnet.C)
}
data = getdata()
data = getdata(100)
data = getdata(100, 0.1)
data_corr = data
getdata <- function(n.samp, sd.val){
x1 <- runif(n.samp,0,1)			# generate predictor variables
x2 <- runif(n.samp,0,1)
x3 <- runif(n.samp,0,1)
x1.sc <- scale(x1)			# center and scale predictor variables
x2.sc <- scale(x2)
x3.sc <- scale(x3)
y_C <- -1*(x1.sc^2) + 0.5*x2.sc + 0.1*x3.sc + rnorm(n.samp,0,sd.val)		# generate response variable y_C
dat <- data.frame(y_C, x1.sc, x2.sc, x3.sc)
return(dat)
}
data <- getdata()
data <- getdata(100, 0.1)
cor(data[,1], data[,2])
cor(data_corr[,1], data_corr[,2])
str(data)
anova(lm(y_C ~ x1.sc + x2.sc + x3.sc))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data_corr))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data_corr))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data))
anova(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data_corr))
summary(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data_corr))
summary(lm(y_C ~ I(x1.sc^2) + x2.sc + x3.sc, data = data))
summary(lm(y_C ~ I(x1.sc^2), data = data))
anova(lm(y_C ~ I(x1.sc^2), data = data))
anova(lm(y_C ~ x2.sc, data = data))
anova(lm(y_C ~ x2.sc, data = data_corr))
anova(lm(y_C ~ x2.sc, data = data_corr))
atan(10)
atan(4/15)
atan(4/15)*57.3
atan(15)*57.3
atan(15/4)*57.3
atan(4/15)
atan(4/15)*57
control_data
setwd('Cash_doors')
source('Cash_doors_scripts/Cashdoors_extraction_control.R')
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') + stat_summary(fun.data = mean_cl_normal, geom = "errorbar")
source('Cash_doors_scripts/Cashdoors_extraction_win.R')
library(gtools)
library(ggplot2)
library(plyr)
data <- smartbind(data_win, data_control)
data$chose_risky <- as.numeric(data$chose_risky)
data$rt <- as.numeric(data$rt)
total_risk_per_participant <- ddply(data, c('participant'), function(subject){
return(data.frame(total_risk = mean(subject$total_risk), condition = subject$condition[1],
outcome_condition = subject$outcome_condition[1],
risk_first_to_mind = subject$risk_first_to_mind[1]))})
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') + stat_summary(fun.data = mean_cl_normal, geom = "errorbar")
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", mult = 1)
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", size = 5)
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = 1)
?stat+summary
?stat_summary
?stat_summary_bin
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", aes(width) = 1)
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", aes(width = 1))
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", aes(width = 0.5))
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", aes(width = 0.2))
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') + geom_errorbar()
ggplot(total_risk_per_participant, aes(condition, total_risk, fill = condition)) +
stat_summary(fun.y = 'mean', geom = 'bar') + geom_errorbar() +
stat_summary(fun.data = mean_cl_normal, geom = "errorbar", aes(width = 0.2))
